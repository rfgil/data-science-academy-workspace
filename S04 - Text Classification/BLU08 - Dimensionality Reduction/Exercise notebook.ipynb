{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7527be455d372803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9bab744c327dc14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "In times where even [facebook goes down](https://www.theguardian.com/technology/2021/oct/04/facebook-instagram-whatsapp-outage-what-to-do), books are our reliable companions. For true book lovers, it's hard to resist the urge to buy huge piles to add to our \"To read\" lists. But how to know which is the right book to buy?\n",
    "\n",
    "In any online book store, you can find hundreds of reviews. You also have [goodreads](https://www.goodreads.com/), [book riot](https://bookriot.com/), and many others.\n",
    "\n",
    "\n",
    "\n",
    "The only problem is, more and more, is hard to navigate through the hundreds and hundreds of reviews, and even if you did have the time, a huge amount is completely unhelpful. Not to mention the ones that definitely seem posted by bots.\n",
    "\n",
    "<img src=\"media/fake-reviews.jpg\" width=\"50%\" />\n",
    "\n",
    "\n",
    "## Q1 - Baseline \n",
    "\n",
    "To cope with all of this, you set out to create a model to look for the most helpful reviews for you. You find a dataset online providing reviews and helpfulness metrics and start there.\n",
    "\n",
    "Load the dataset and check its structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6273f8aa182f664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Bear; Kevin Henkes (2008) Harper Collins C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This book is based on the sad truth of the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>When Lara's grandmother's ghost begins bashing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Charlotte Simmons is the definitive, classic A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Much too stereotypical for my tastes and the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpfulness                                         reviewText\n",
       "0            1  Old Bear; Kevin Henkes (2008) Harper Collins C...\n",
       "1            0  This book is based on the sad truth of the sta...\n",
       "2            1  When Lara's grandmother's ghost begins bashing...\n",
       "3            1  Charlotte Simmons is the definitive, classic A...\n",
       "4            0  Much too stereotypical for my tastes and the s..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads dataframe of reviews helpfulness \n",
    "    \"\"\"\n",
    "    df = pd.read_csv('datasets/book_review_helpfulness.csv')\n",
    "    df.head()\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0f698ca70b9e015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.a)\n",
    "\n",
    "First thing you decide is to extract features from the reviews. So you start working out how to do it. First you think it may be good to preprocess the text a bit, so you look into a few options. You end up converging into the following:\n",
    "\n",
    "- tokenizing the text, using WordPunctTokenizer\n",
    "- lowercasing the text\n",
    "\n",
    "You then decide to feed this into a CountVectorizer. Implement this function below: \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1492b0bfab58773c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(X_train, X_test):\n",
    "    \"\"\"Converts the provided text training and test data into \n",
    "    feature counts. Additionally, returns the used vectorizer, \n",
    "    the processed dataframes and the number of features extracted\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: dataframe: training data, containing a \"reviewText\" column\n",
    "        X_test: dataframe: test data, containing a \"reviewText\" column\n",
    "    \n",
    "    Returns:\n",
    "        vectorizer: used count vectorizer \n",
    "        num_features: vectorizer actual number of features, for sanity check\n",
    "        X_train_vec: processed training features\n",
    "        X_test_vec: processed test features\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    preprocessor = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc.lower()))\n",
    "    \n",
    "    X_train['reviewText'] = X_train['reviewText'].map(preprocessor)\n",
    "    X_test['reviewText'] = X_test['reviewText'].map(preprocessor)\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train.reviewText)\n",
    "    X_test_vec = vectorizer.transform(X_test.reviewText)\n",
    "    \n",
    "    num_features = X_train_vec.shape[1]\n",
    "    \n",
    "    return vectorizer, num_features, X_train_vec, X_test_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-761d12c2e662383e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['helpfulness']), \n",
    "    df['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test)\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[12, :]), 284)\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[300, :]), 157)\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[1411, :]), 222)\n",
    "\n",
    "assert num_features == 44103\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c6d3c1f721b3d73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_For the next exercises we'll provide you a different version of the preprocessed data._\n",
    "\n",
    "_Load the preprocessed dataset provided in the file_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efe125f9b6c0960e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>old bear kevin henkes harper collins childrenl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>book based sad truth state country humor berni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>lara grandmother ghost begins bashing guests f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>charlotte simmons definitive classic american ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>much stereotypical tastes storyline unbelievab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpfulness                                         reviewText\n",
       "0            1  old bear kevin henkes harper collins childrenl...\n",
       "1            0  book based sad truth state country humor berni...\n",
       "2            1  lara grandmother ghost begins bashing guests f...\n",
       "3            1  charlotte simmons definitive classic american ...\n",
       "4            0  much stereotypical tastes storyline unbelievab..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\"\n",
    "    Loads dataframe of preprocessed helpfulness review \n",
    "    \"\"\"\n",
    "    preprocessed_df = pd.read_csv('datasets/book_review_helpfulness_preprocessed.csv')\n",
    "    preprocessed_df.head()\n",
    "    return preprocessed_df\n",
    "\n",
    "df_preprocessed = load_preprocessed_data()\n",
    "df_preprocessed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28fc4a869d434823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.b)\n",
    "\n",
    "You now want to use your newly found features to build a baseline. Create a function that receives your vectorized instances and train a naïve bayes model.\n",
    "\n",
    "First we'll start by defining a function to obtain precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93c0f5ba86c7ff62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall(y_test, y_pred):\n",
    "    \"\"\"Returns the precision and recall of the helpfulness class (label = 1)\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        y_pred (Series): Predictions corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score of the helpfulness class (1) on the test data\n",
    "        recall (float): The recall score of the helpfulness class (1) on the test data\n",
    "    \"\"\"\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "    return precision, recall\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a99d737525b250d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement below the function ot train a naive bayes model and use the precision_recall function to return these metrics over the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4212f8a48fad26c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"Returns a fitted Multinomial Naive Bayes model, the predictions on the test set\n",
    "    and the precision and recall scores for these predictions\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (Series): Vectorized text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test_vec (Series): Vectorized text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        clf (MultinomialNB): MultinomialNB classifier fitted to the feature-selected training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "        precision (float): The precision score of the helpfulness class (1) on the test data\n",
    "        recall (float): The recall score of the helpfulness class (1) on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    clf = MultinomialNB()\n",
    "    \n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    \n",
    "    precision = precision_score(y_true = y_test, y_pred=y_pred)\n",
    "    recall = recall_score(y_true = y_test, y_pred=y_pred)\n",
    "\n",
    "    return clf, y_pred, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb35e27414e100a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fac3a6ee5b15c471",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['helpfulness'], test_size=0.2, random_state=42)\n",
    "\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "assert math.isclose(precision, 0.6305625524769102)\n",
    "assert math.isclose(recall, 0.689623507805326)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['helpfulness'], test_size=0.5, random_state=42)\n",
    "\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "assert math.isclose(precision, 0.6058282208588958)\n",
    "assert math.isclose(recall, 0.7391467065868264)\n",
    "\n",
    "df_prep = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_prep['reviewText'], df_prep['helpfulness'], test_size=0.2, random_state=42)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "assert math.isclose(precision, 0.6260032102728732), precision\n",
    "assert math.isclose(recall, 0.7162534435261708), recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1124edb4314d1b0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now run the two functions you built put together and check your baseline scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1ed06f12f0f2d2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline precision: 0.6274292059966685\n",
      "Baseline recall: 0.7040498442367601\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['helpfulness']), \n",
    "    df['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test)\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "print(f\"Baseline precision: {precision}\")\n",
    "print(f\"Baseline recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-652e101fb2ad549e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_We'll also run with our own provided preprocessed data so you have a base of comparison._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f051bf3811e71cdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 43290\n",
      "Baseline precision: 0.6101159114857745\n",
      "Baseline recall: 0.7214953271028037\n"
     ]
    }
   ],
   "source": [
    "df_prep = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_prep['reviewText'], \n",
    "    df_prep['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "print(f\"Number of features: {len(cv.vocabulary_)}\")\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "print(f\"Baseline precision: {precision}\")\n",
    "print(f\"Baseline recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a27675ff9718a6fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we'll use the provided version on your exercises consider this as the baseline:\n",
    "\n",
    "* Baseline precision: 0.6101159114857745\n",
    "* Baseline recall: 0.7214953271028037\n",
    "\n",
    "\n",
    "## Q2) Feature analysis and selection\n",
    "\n",
    "Those are not bad results to start with, but as you've learned about features selection you want to try it out. After all, 43 thousand feature is still a pretty big number. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c58f1a74e4aeb4c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "\n",
    "You start by using model based feature selection. For that purpose, you want to use the capability of TF-IDF/CountVectorizer to select the features to use. \n",
    "\n",
    "\n",
    "### Q2.a)\n",
    "\n",
    "Fit a TF-IDF vectorizer with a specific number of features. Note that the features will be selected based on term frequency, so whatever vocabulary used will be based on the term count. Return these features sorted inverse document frequency, the other measure used by TF-IDF - as you've seen in the learning notebooks, a measure of whether a term is common or rare in the documents.\n",
    "\n",
    "Create that function below:\n",
    "* train a tf-idf vectorizer on the provided train data (passing a parameter to define the maximum number of features to use)\n",
    "* return its features sorted by their inverse document frequency (in descending order). \n",
    "\n",
    "**Note**: In case of tie, this is, if several features have the same IDF score, the features should be sorted by alphabetical order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d22a3960382cfe73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_tfidf_ngrams_sorted_by_idf(X_train, top_features=30):\n",
    "    \"\"\"Fits a TfidfVectorizer and returns its features sorted by \n",
    "    idf score\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Vectorized text data for training\n",
    "        top_features: Top features to use \n",
    "    \n",
    "    Returns:\n",
    "        vectorizer: used tf-idf vectorizer \n",
    "        ngrams_sorted (list): The ngrams of fitted_vectorizer sorted in descending order\n",
    "                              by their idf score. In case of tie, the features should be sorted \n",
    "                              by alphabetical order\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=top_features)\n",
    "    \n",
    "    vectorizer.fit_transform(X_train)\n",
    "\n",
    "    ngrams_sorted = vectorizer.get_feature_names()\n",
    "\n",
    "    mydict = {}\n",
    "    for feature in vectorizer.get_feature_names():\n",
    "        mydict[feature] = vectorizer.idf_[vectorizer.vocabulary_[feature]]\n",
    "    \n",
    "       \n",
    "    ngrams_sorted = [ x[0] for x in sorted(mydict.items(), key=lambda x: x[1], reverse=True) ]\n",
    "\n",
    "    return vectorizer, ngrams_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8188311a1f32f69",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "vectorizer, sorted_n_grams = get_tfidf_ngrams_sorted_by_idf(df_preprocessed['reviewText'], top_features=100)\n",
    "assert hashlib.sha256(sorted_n_grams[12].encode()).hexdigest() == \"bfa062de040f55a15ce910800757061ec3d2fc31d6b7c72d9fa02b75a9ad1133\"\n",
    "\n",
    "vectorizer, sorted_n_grams = get_tfidf_ngrams_sorted_by_idf(df_preprocessed['reviewText'], top_features=100)\n",
    "assert hashlib.sha256(sorted_n_grams[41].encode()).hexdigest() == \"47951833ecf6211793c07d246c29b2314abdd784d806d2253c5723dc5e3b0d74\"\n",
    "\n",
    "vectorizer, sorted_n_grams = get_tfidf_ngrams_sorted_by_idf(df_preprocessed['reviewText'], top_features=100)\n",
    "assert hashlib.sha256(sorted_n_grams[76].encode()).hexdigest() == \"686f746a95b6f836d7d70567c302c3f9ebb5ee0def3d1220ee9d4e9f34f5e131\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-adca65f62fff79d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the features with top inverse document frequency in your vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd01e244d60c3db1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ben', 'jane', 'empire', 'brown', 'henry', 'mary', 'peter', 'france', 'joe', 'economic', 'vampire', 'jesus', 'harry', 'film', 'scientific', 'animals', 'french', 'queen', 'philosophy', 'music', 'army', 'civil', 'club', 'george', 'market', 'theory', 'team', 'conflict', 'london', 'nation']\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#show most specifc 30 features of the 100 selected features\n",
    "vectorizer, sorted_n_grams = get_tfidf_ngrams_sorted_by_idf(X_train, top_features=1000)\n",
    "\n",
    "print(sorted_n_grams[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e6234e9cbfdcc8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we're only seeing the top IDF features, they tend to be more specific words you would find only in a subset of documents. But in general all of these features are not surprising in the context of books. \n",
    "\n",
    "### Q2.b)\n",
    "\n",
    "Finally we want to see how this behaves in comparison with our baseline. Use the functions you have seen above to try out different values of features (50, 100, 500, 1000, 2000, 5000 and 10000) in the training of a Multinomial Naive Bayes. Check the precision and recall of each run and store it on a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-641b26a61917cf52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# For the following values: 50, 100, 500, 1000, 2000, 5000 and 10000\n",
    "# 1. Fit a TF-IDF limiting its features \n",
    "# 2. Train a Naive Bayes model\n",
    "# 3. Obtain precision and recall\n",
    "# 4. Store the lists of feature length, precision and recall values \n",
    "\n",
    "feature_lengths = []\n",
    "precisions_values = []\n",
    "recall_values = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for k in [50, 100, 500, 1000, 2000, 5000, 10000]:\n",
    "    vectorizer, ngrams_sorted = get_tfidf_ngrams_sorted_by_idf(X_train, k)\n",
    "    \n",
    "    X_train_vec = vectorizer.transform(X_train) \n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "    \n",
    "    feature_lengths.append(k)\n",
    "    precisions_values.append(precision)\n",
    "    recall_values.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d190a2151e63bd0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "assert len(feature_lengths) == len(precisions_values) == len(recall_values) == 7\n",
    "\n",
    "assert math.isclose(precisions_values[4], 0.6114101184068891)\n",
    "assert math.isclose(recall_values[5], 0.7171339563862928)\n",
    "\n",
    "assert math.isclose(np.sum(precisions_values), 4.170958590158336)\n",
    "assert math.isclose(np.sum(recall_values), 5.242990654205607)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fda55a7260224715",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Look at the precision, recall variation of each and answer the questions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-44d6983aaea90cc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50\n",
      "Precision: 0.5735359856951274\n",
      "Recall: 0.7993769470404984\n",
      "==============================\n",
      "Number of features: 100\n",
      "Precision: 0.5740824648844586\n",
      "Recall: 0.7894080996884735\n",
      "==============================\n",
      "Number of features: 500\n",
      "Precision: 0.5925\n",
      "Recall: 0.7383177570093458\n",
      "==============================\n",
      "Number of features: 1000\n",
      "Precision: 0.6041666666666666\n",
      "Recall: 0.7227414330218068\n",
      "==============================\n",
      "Number of features: 2000\n",
      "Precision: 0.6114101184068891\n",
      "Recall: 0.7077881619937695\n",
      "==============================\n",
      "Number of features: 5000\n",
      "Precision: 0.6132125732551944\n",
      "Recall: 0.7171339563862928\n",
      "==============================\n",
      "Number of features: 10000\n",
      "Precision: 0.60205078125\n",
      "Recall: 0.7682242990654206\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for n_features, precision, recall in zip(feature_lengths, precisions_values, recall_values):\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Precision: {precision}\")    \n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1192b685dad4fab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.i)** Which number of feature yielded the **highest precision**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb3a3de21e03e7f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features_higher_precision = 5000 # numeric value\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0dd5a122b8b0df3a",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(bytes(num_features_higher_precision)).hexdigest() == \\\n",
    "    \"7ca5bd879f393d9dd05b14f38add9c0fc6b67928f7f2d261b2e47a32ee8219e3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2022411c99d37b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.ii)** Which number of feature yielded the **highest recall**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98f7382e6d605724",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features_higher_recall = 50 # numeric value\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f164bd4d459f2790",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(bytes(num_features_higher_recall)).hexdigest() == \\\n",
    "    \"cc2786e1f9910a9d811400edcddaf7075195f7a16b216dcbefba3bc7c4f2ae51\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01ec636c599a5df8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.iii)** Which number of feature yielded the **highest F1-score**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd0c030243f5bfc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features_higher_f1 = 10000 # numeric value\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cb3beeb2e54d7fe1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(bytes(num_features_higher_f1)).hexdigest() == \\\n",
    "    \"95b532cc4381affdff0d956e12520a04129ed49d37e154228368fe5621f0b9a2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1043b46f217fa6c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.iv)** Knowing we won't be able to read that many reviews, but we want to make sure the ones we do read are helpfull, which number of features would you choose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fcf4ab6fc48580e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "choice_model = \"precision\"  # one of \"precision\" or \"recall\"  or \"f1-score\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f5fb9e31f8c9a160",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(choice_model.encode()).hexdigest() == \\\n",
    "    \"68c2f9ee314749c05c96df0cad305b0972506d78bb9b23c942cf805b274236c6\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28f6e998fa1a40a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "### Q2.c)\n",
    "\n",
    "You also heard about Chi squared, another method that will try to find the relevance of features and select the best ones based on it.\n",
    "\n",
    "<img src=\"media/chi-squared-not-sure.jpg\" width=\"40%\" />\n",
    "\n",
    "You decide to implement a function to run chi-squared over your vectorizer and return the selected features.\n",
    "\n",
    "**Note**: In this case, don't limit CountVectorizer/TfidfVectorizer to any number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0c66b2edf797342",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_chi2(X_train, y_train, top_features=100, vectorizer_type=\"count\"):\n",
    "    \"\"\"Converts the provided text training data into TF-IDF feature counts. \n",
    "    Additionally, selects the best features with chi squared method and returns\n",
    "    the sorted ngrams\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: training data\n",
    "        y_train: training labels\n",
    "        top_features: maximum number of features to use\n",
    "        vectorizer_type: type of vectorizer to use (\"count\" or \"tfidf\")\n",
    "\n",
    "    Returns:\n",
    "        vectorizer: used tf-idf vectorizer \n",
    "        ch2: used feature selector\n",
    "        X_train_ch2: transformed vector after feature selection\n",
    "        ngrams_sorted (list): The top features of fitted_vectorizer sorted in ascending order\n",
    "                              by their chi squared score\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    if vectorizer_type == \"count\":\n",
    "        vectorizer = CountVectorizer()\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    ch2 = SelectKBest(chi2, k=top_features)\n",
    "    X_train_ch2 = ch2.fit_transform(X_train_vec, y_train)\n",
    "    \n",
    "    mydict = {}\n",
    "    for feature_index in ch2.get_support(indices=True):\n",
    "        mydict[feature_names[feature_index]] = ch2.scores_[feature_index]\n",
    "       \n",
    "    ngrams_sorted = [ x[0] for x in sorted(mydict.items(), key=lambda x: x[1], reverse=True) ]\n",
    "\n",
    "    return vectorizer, ch2, X_train_ch2, ngrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0b7f52fc1fdef287",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features = extract_features_chi2(X_train, y_train, vectorizer_type=\"tfidf\")\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[12, :]), 0.30615842862660636)\n",
    "assert ch2.k == 100\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features = extract_features_chi2(X_train, y_train, vectorizer_type=\"tfidf\", top_features=123)\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[:, 122]), 33.74087286012022)\n",
    "assert ch2.k == 123\n",
    "\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features = extract_features_chi2(X_train, y_train, vectorizer_type=\"count\")\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[12, :]), 26), np.sum(X_train_ch2.todense()[12, :])\n",
    "assert ch2.k == 100\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features  = extract_features_chi2(X_train, y_train, vectorizer_type=\"count\", top_features=123)\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[:, 122]), 734)\n",
    "assert ch2.k == 123\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15b6c9135c1144ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's apply our extractor with 30 features and check them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de4f48b0315a5c15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'story',\n",
       " 'one',\n",
       " 'life',\n",
       " 'quot',\n",
       " 'also',\n",
       " 'world',\n",
       " 'well',\n",
       " 'first',\n",
       " 'people',\n",
       " 'novel',\n",
       " 'time',\n",
       " 'years',\n",
       " 'even',\n",
       " 'reader',\n",
       " 'many',\n",
       " 'two',\n",
       " 'mother',\n",
       " 'make',\n",
       " 'young',\n",
       " 'new',\n",
       " 'like',\n",
       " 'much',\n",
       " 'work',\n",
       " 'find',\n",
       " 'stories',\n",
       " 'may',\n",
       " 'home',\n",
       " 'family',\n",
       " 'us']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features  = extract_features_chi2(X_train, y_train, top_features=30)\n",
    "\n",
    "most_important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-099ecaf666f3e62e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also see how often each of the features selected appears in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75fa0016fc29f7e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents that contains the word(s) \"book\"\n",
      "----\n",
      "1    3063\n",
      "0    2507\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"story\"\n",
      "----\n",
      "1    1705\n",
      "0    1164\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"one\"\n",
      "----\n",
      "1    2537\n",
      "0    1710\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"life\"\n",
      "----\n",
      "1    1045\n",
      "0     565\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"quot\"\n",
      "----\n",
      "1    272\n",
      "0    134\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"also\"\n",
      "----\n",
      "1    937\n",
      "0    522\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"world\"\n",
      "----\n",
      "1    745\n",
      "0    375\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"well\"\n",
      "----\n",
      "1    1224\n",
      "0     710\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"first\"\n",
      "----\n",
      "1    1079\n",
      "0     634\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"people\"\n",
      "----\n",
      "1    839\n",
      "0    487\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"novel\"\n",
      "----\n",
      "1    835\n",
      "0    484\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"time\"\n",
      "----\n",
      "1    1554\n",
      "0     943\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"years\"\n",
      "----\n",
      "1    633\n",
      "0    310\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"even\"\n",
      "----\n",
      "1    1289\n",
      "0     735\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"reader\"\n",
      "----\n",
      "1    785\n",
      "0    404\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"many\"\n",
      "----\n",
      "1    952\n",
      "0    544\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"two\"\n",
      "----\n",
      "1    730\n",
      "0    399\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"mother\"\n",
      "----\n",
      "1    316\n",
      "0    146\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"make\"\n",
      "----\n",
      "1    1087\n",
      "0     591\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"young\"\n",
      "----\n",
      "1    448\n",
      "0    198\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"new\"\n",
      "----\n",
      "1    919\n",
      "0    514\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"like\"\n",
      "----\n",
      "1    1668\n",
      "0    1158\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"much\"\n",
      "----\n",
      "1    1080\n",
      "0     710\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"work\"\n",
      "----\n",
      "1    985\n",
      "0    519\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"find\"\n",
      "----\n",
      "1    824\n",
      "0    450\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"stories\"\n",
      "----\n",
      "1    411\n",
      "0    224\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"may\"\n",
      "----\n",
      "1    667\n",
      "0    378\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"home\"\n",
      "----\n",
      "1    370\n",
      "0    152\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"family\"\n",
      "----\n",
      "1    435\n",
      "0    214\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"us\"\n",
      "----\n",
      "1    2793\n",
      "0    1836\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in most_important_features:\n",
    "    print('Documents that contains the word(s) \"%s\"' % feature)\n",
    "    print('----')\n",
    "    docs = X_train.str.lower().str.contains(feature)\n",
    "    print(str(y_train[docs].value_counts()) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1563c9a666e59d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It seems that most features selected as relevant are slightly more attached to the \"helpful\" class (1).\n",
    "\n",
    "Now let's see how that translates in terms of model training. Repeat the procedure you done above to check the feature selection scores. Do that using the function you just created with count vectorizer and chi-squared:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38b468714a5ca798",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# For the following values: 50, 100, 500, 1000, 2000, 5000 and 10000\n",
    "# 1. Use a count vectorizer and ch2 feature selection with the give number of features \n",
    "# 2. Train a Naive Bayes model\n",
    "# 3. Obtain precision and recall\n",
    "# 4. Store the lists of feature length, precision and recall values \n",
    "\n",
    "feature_lengths = []\n",
    "precisions_values = []\n",
    "recall_values = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for k in [50, 100, 500, 1000, 2000, 5000, 10000]:\n",
    "    vectorizer, ch2, X_train_ch2, ngrams_sorted = extract_features_chi2(X_train, y_train, k)\n",
    "    \n",
    "    #X_train_vec = vectorizer.transform(X_train)\n",
    "    X_test_ch2 = ch2.transform(vectorizer.transform(X_test))\n",
    "    \n",
    "    clf, y_pred, precision, recall = train_model_naive_bayes(X_train_ch2, y_train, X_test_ch2, y_test)\n",
    "    \n",
    "    feature_lengths.append(k)\n",
    "    precisions_values.append(precision)\n",
    "    recall_values.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0ad99d25ab050a2f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "assert len(feature_lengths) == len(precisions_values) == len(recall_values) == 7\n",
    "\n",
    "assert math.isclose(precisions_values[4], 0.649458784346378)\n",
    "assert math.isclose(recall_values[5], 0.49221183800623053)\n",
    "\n",
    "assert math.isclose(np.sum(precisions_values), 4.416826519514508)\n",
    "assert math.isclose(np.sum(recall_values), 3.5514018691588785)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-760cb8df07ac60fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, check the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50\n",
      "Precision: 0.5874079035498996\n",
      "Recall: 0.5464174454828661\n",
      "==============================\n",
      "Number of features: 100\n",
      "Precision: 0.6043795620437956\n",
      "Recall: 0.5158878504672897\n",
      "==============================\n",
      "Number of features: 500\n",
      "Precision: 0.630016051364366\n",
      "Recall: 0.48909657320872274\n",
      "==============================\n",
      "Number of features: 1000\n",
      "Precision: 0.6371753246753247\n",
      "Recall: 0.48909657320872274\n",
      "==============================\n",
      "Number of features: 2000\n",
      "Precision: 0.649458784346378\n",
      "Recall: 0.48598130841121495\n",
      "==============================\n",
      "Number of features: 5000\n",
      "Precision: 0.661641541038526\n",
      "Recall: 0.49221183800623053\n",
      "==============================\n",
      "Number of features: 10000\n",
      "Precision: 0.6467473524962178\n",
      "Recall: 0.5327102803738317\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for n_features, precision, recall in zip(feature_lengths, precisions_values, recall_values):\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Precision: {precision}\")    \n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1334fe8f60c2d3fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this case, we are able to raise precision (check the values for 5000 features) even though we sacrifice recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8be788c4e7462464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3.\n",
    "\n",
    "Now let's move on to more complex methods. In the previous methods, the selection is based on the actual n-grams, so we are limited to the information each feature brings separately. We will now try PCA, which will try to find the principal components of our vectorized representation \n",
    "\n",
    "Write a function that computes PCA on top of a CountVectorizer. Additionally, train a Support Vector Classifier on top of the PCA output.\n",
    "\n",
    "**To avoid using up too much memory, and as we'll use dense matrices, please use a max_features value of 5000 in the vectorizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c323629aaea537dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42):\n",
    "    \"\"\"Returns a fitted TfidfVectorizer, the truncated svd used, a support vector classifier\n",
    "    and the test predictions computed with these\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        num_features (int): maximum number of features to use\n",
    "        seed (int): Seed to use for random state\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (CountVectorizer): CountVectorizer, fitted to X_train\n",
    "        pca (PCA): PCA with provided number of features as components\n",
    "        clf (SVC): SVC classifier fitted to the feature-selected training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train_vec = vectorizer.transform(X_train).toarray()\n",
    "    X_test_vec = vectorizer.transform(X_test).toarray()\n",
    "    data_var = np.var(X_train_vec, axis=0).sum()\n",
    "    \n",
    "    \n",
    "    pca = PCA(n_components=num_features, random_state=seed)\n",
    "    pca.fit(X_train_vec)\n",
    "\n",
    "    X_train_pca = pca.transform(X_train_vec)\n",
    "    X_test_pca = pca.transform(X_test_vec)\n",
    "    \n",
    "    \n",
    "    clf =  SVC(random_state=seed)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    \n",
    "    explained_variance = 1.0*np.var(X_train_pca, axis=0).sum() / data_var\n",
    "    \n",
    "    return vectorizer, pca, clf, y_pred, explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e0c931cb07614c23",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Computed PCA for 30 features -----\n",
      "\n",
      "----- Computed PCA for 50 features -----\n",
      "\n",
      "----- Computed PCA for 100 features -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer, pca, clf, y_pred_30, variance_30 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=30, seed=42)\n",
    "print(\"----- Computed PCA for 30 features -----\\n\")\n",
    "assert np.mean(y_pred_30) == 0.5516666666666666\n",
    "assert np.sum(y_pred_30) == 1655\n",
    "assert pca.n_components == 30\n",
    "assert vectorizer.get_feature_names()[4424] == 'teaching'\n",
    "\n",
    "vectorizer, pca, clf, y_pred_50, variance_50 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=50, seed=42)\n",
    "print(\"----- Computed PCA for 50 features -----\\n\")\n",
    "assert np.mean(y_pred_50) == 0.5416666666666666\n",
    "assert np.sum(y_pred_50) == 1625\n",
    "assert pca.n_components == 50\n",
    "assert vectorizer.get_feature_names()[4424] == 'teaching'\n",
    "\n",
    "vectorizer, pca, clf, y_pred_100, variance_100 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42)\n",
    "print(\"----- Computed PCA for 100 features -----\\n\")\n",
    "assert np.mean(y_pred_100) == 0.544\n",
    "assert np.sum(y_pred_100) == 1632\n",
    "assert pca.n_components == 100\n",
    "assert vectorizer.get_feature_names()[4424] == 'teaching'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1ecb5e0f82ec689",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can obtain the precision and recall as before and compare with the previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4362d8b3356bd257",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions with 30 features: \n",
      "Precision: 0.656797583081571\n",
      "Recall: 0.6772585669781932\n",
      "\n",
      "Predictions with 50 features: \n",
      "Precision: 0.6646153846153846\n",
      "Recall: 0.6728971962616822\n",
      "\n",
      "Predictions with 100 features: \n",
      "Precision: 0.6611519607843137\n",
      "Recall: 0.6722741433021807\n"
     ]
    }
   ],
   "source": [
    "precision, recall = get_precision_recall(y_test, y_pred_30)     \n",
    "print(\"\\nPredictions with 30 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_50)      \n",
    "print(\"\\nPredictions with 50 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_100)     \n",
    "print(\"\\nPredictions with 100 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98437874ea1e4117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nice! We keep the high precision of other feature selection methods but with much higher recall!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b17e715e9161796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4.\n",
    "\n",
    "Now we'll change gears a bit and look into word vectors. In Learning Notebook 3 we told you that word vectors can be visualized after being projected into 2D space, and we showed you this diagram:\n",
    "\n",
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">\n",
    "\n",
    "Now we'll try to combine what you've learned about word embeddings and PCA to make our own visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3847eb100057a7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b982ddaa87eec51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.a)\n",
    "\n",
    "First, to get comfortable with spacy, get the vector for the word \"book\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34ae448736fdde6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "book_vector = nlp('book').vector\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cdd4b59c690522a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(book_vector.sum(), -8.673375, abs_tol=0.00001)\n",
    "assert book_vector.shape[0] == 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7566702b1fd9755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.b) \n",
    "\n",
    "Next, write a function that uses sklearn's PCA to reduce our vectors to a convenient number of dimensions for plotting. The function should return the reduced dimension word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c4ffc6d44920880",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_word_vecs(vectors, random_state):\n",
    "    \"\"\"\n",
    "    Returns PCA-reduced word vectors of the input vectors for plotting\n",
    "    \n",
    "    Parameters:\n",
    "        vectors (np.array): Word vectors to be reduced\n",
    "        random_state (int): random state to use in PCA\n",
    "\n",
    "    Returns:\n",
    "        reduced_vecs (np.array): Word vectors reduced to the number of dimensions\n",
    "                                 suitable for plotting with given random_state\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    pca.fit(vectors)\n",
    "    reduced_vecs = pca.transform(vectors)\n",
    "\n",
    "    return reduced_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05bdaa52733db9e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_vectors = np.array([[0.1, 0.2, 0.3, 0.4], [0.3, 0.5, 0.1, 0.7], [0.8, 0.6, 0.2, 0.4]])\n",
    "reduced_vecs = reduce_word_vecs(test_vectors, random_state=42)\n",
    "\n",
    "assert reduced_vecs.shape == (3,2)\n",
    "assert math.isclose(reduced_vecs[1][1], 0.24736592153367926)\n",
    "assert math.isclose(reduced_vecs[2][0], 0.43388622222454437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f75d7362f3b5b18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we'll create an array of ~75,000 of spacy's word vectors and use your PCA function to reduce them. If you're curious about using the full amount of word vectors, you can change the code to iterate over all vocab words - `list(nlp.vocab.strings)` - instead of our own `vocab_strings`, but beware it will use a lot of memory!\n",
    "\n",
    "We'll also set a list of words that we'll plot later and force our set of vectors to include these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d7c693a88480b94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words_to_plot = ['banana', 'pineapple', 'mango', 'red', 'blue', 'yellow', 'woman', 'man', 'child', 'playing',\n",
    "                 'reading', 'studying', 'nintendo', 'sony', 'xbox', 'sad', 'angry', 'bored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-644c6807023bc77f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open('datasets/word_subset.txt') as fwords:\n",
    "    vocab_strings = fwords.read().splitlines()\n",
    "\n",
    "full_vocab_vecs = []\n",
    "for tok in vocab_strings:\n",
    "    full_vocab_vecs.append(nlp.vocab.get_vector(tok))\n",
    "\n",
    "vocab_array = np.array(full_vocab_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors shape pre-PCA: (103202, 300)\n",
      "Word vectors shape after PCA: (103202, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Word vectors shape pre-PCA: {}'.format(vocab_array.shape))\n",
    "\n",
    "full_vocab_reduced = reduce_word_vecs(vocab_array, random_state=42)\n",
    "\n",
    "print('Word vectors shape after PCA: {}'.format(full_vocab_reduced.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-93396749fc10fb39",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(full_vocab_reduced[100][0], -3.0880196, abs_tol=0.00001)\n",
    "assert math.isclose(full_vocab_reduced[9999][0], -0.9480563, abs_tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0feaee69a29da92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.c)\n",
    "\n",
    "Time to plot! For this, we'll limit the visualized words to a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9415c85fbda3dbb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "coords = []\n",
    "for word in words_to_plot:\n",
    "    idx = vocab_strings.index(word)\n",
    "    coords.append(full_vocab_reduced[idx])\n",
    "\n",
    "coords_array = np.array(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7c059bc4a449b27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x640 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.xlim(min([x for x in coords_array[:,0]]), max([x for x in coords_array[:,0]]))\n",
    "plt.ylim(min([y for y in coords_array[:,1]]), max([y for y in coords_array[:,1]]))\n",
    "plt.scatter(coords_array[:,0], coords_array[:,1])\n",
    "\n",
    "for item, x, y in zip(words_to_plot, coords_array[:,0], coords_array[:,1]):\n",
    "    plt.annotate(item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                 ha='right', va='bottom', color='purple', fontsize=14 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dcc429eed65a7f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The PCA seems to have worked! In the diagram we can see similar types of words closer together. But of course, take these visualizations with a grain of salt because it is practically impossible to preserve all distances in a high dimensional space in just 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a980c450673e4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.d) \n",
    "\n",
    "As a final exercise, we'll look at some word similarities.\n",
    "\n",
    "Write a function that returns the next closest word in terms of cosine similarity to a given word. If there are multiple words with the same highest similarity, return all of them.\n",
    "\n",
    "Hint: you can use the already-imported `cosine_similarity` function from sklearn to compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72bc3a66c2a8f550",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def closest_word(input_word, words_in_vocab, word_vectors):\n",
    "    '''Returns a list of the closest word or words to the input word, based on cosine similarities\n",
    "       of the word vectors given\n",
    "    \n",
    "    Parameters:\n",
    "        input_word (string): Search for the closest words to this word\n",
    "        words_in_vocab (list): Vocabulary associated with the vectors in word_vectors\n",
    "        word_vectors (np.array): Word vectors associated with the strings in words_in_vocab\n",
    "\n",
    "    Returns:\n",
    "        closest_words_list (list): List of strings containing the closest word or words to\n",
    "                                   input_word, based on cosine similarities of the word_vectors\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #for word in words_in_vocab:\n",
    "    index = words_in_vocab.index(input_word)\n",
    "    similarities = cosine_similarity(word_vectors)[index]\n",
    "\n",
    "    current_simil = 0\n",
    "    current_index = -1\n",
    "    for i in range(len(similarities)):\n",
    "        if i != index and similarities[i] > current_simil:\n",
    "            current_simil = similarities[i]\n",
    "            current_index = i\n",
    "        \n",
    "    closest_words_list = [ words_in_vocab[current_index] ]\n",
    "        \n",
    "    return closest_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d06a90ba7d38c797",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(hashlib.sha256(closest_word('nintendo', words_to_plot, coords_array)[0].encode()).hexdigest() == \"f711da60664c04c146d7a47b722c38a8d0bf46c3f52c2084c5c8d1cb78138e73\")\n",
    "assert(hashlib.sha256(closest_word('playing', words_to_plot, coords_array)[0].encode()).hexdigest() == \"435c149cbc6a5e5cc373cd33347d4c336a22160e06b7df61092b66e56f4d55ec\")\n",
    "assert(hashlib.sha256(closest_word('pineapple', words_to_plot, coords_array)[0].encode()).hexdigest() == \"6815f3c300383519de8e437497e2c3e97852fe8d717a5419d5aafb00cb43c494\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Closest words to nintendo:\n",
      "['xbox']\n",
      "\n",
      "Closest words to playing:\n",
      "['studying']\n",
      "\n",
      "Closest words to pineapple:\n",
      "['mango']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClosest words to nintendo:\")\n",
    "print(closest_word('nintendo', words_to_plot, coords_array))\n",
    "\n",
    "print(\"\\nClosest words to playing:\")\n",
    "print(closest_word('playing', words_to_plot, coords_array))\n",
    "\n",
    "print(\"\\nClosest words to pineapple:\")\n",
    "print(closest_word('pineapple', words_to_plot, coords_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fdffae6311cd8b15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That is all, see you in the next specialization. \n",
    "\n",
    "PS: feel free to share your book reviews and recommendations\n",
    "\n",
    "<img src=\"media/dont-buy-more-books.jpg\" width=\"50%\" />\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
