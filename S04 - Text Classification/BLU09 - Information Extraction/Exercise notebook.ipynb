{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a3dc8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f881c7002cca9b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU09 - Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e125f899",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4efa4dd0f5a58872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing needed packages here\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def _hash(s):\n",
    "    return hashlib.sha256(\n",
    "        bytes(str(s), encoding='utf8'),\n",
    "    ).hexdigest()\n",
    "\n",
    "cpu_count = int(os.cpu_count()) if os.cpu_count() != None else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a882aedc21b3fc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this learning unit you are going to tackle with a quite real problem: **Detecting fake news!** Let's create a binary classifier to determine if a piece of news is considered 'reliable' or 'unreliable'. You will start by building some basic features, then go on to build more complex ones, and finally putting it all together. You should be able to have a working classifier by the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255edbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b947ffb2b30ff870",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we will be using is the [Fake News](https://www.kaggle.com/c/fake-news/overview) from Kaggle. Each piece of news is either reliable or trustworthy, '0', or unreliable and possibly fake, '1'. First, let's load it up and see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c4d9f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62fce116d684ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didnâ€™t Even See Comeyâ€™s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didnâ€™t Even See Comeyâ€™s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title              author  \\\n",
       "id                                                                          \n",
       "0   House Dem Aide: We Didnâ€™t Even See Comeyâ€™s Let...       Darrell Lucus   \n",
       "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                 text  label  \n",
       "id                                                            \n",
       "0   House Dem Aide: We Didnâ€™t Even See Comeyâ€™s Let...      1  \n",
       "1   Ever get the feeling your life circles the rou...      0  \n",
       "2   Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3   Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4   Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"datasets/fakenews/train.csv\"\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "df = df[:5000]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40814b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38f5eb775841d955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see that we have 4 columns that are pretty self-explanatory, let's drop the author column since we only want to practice our text analysis, drop title as well for simplicity sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2118e6f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4555d789c2c7417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[\"author\", \"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6313691e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f6782b20750823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:33<00:00, 32.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[House Dem Aide: We Didnâ€™t Even See Comeyâ€™s Letter Until Jason Chaffetz Tweeted It By Darrell Lucus on October 30, 2016 Subscribe Jason Chaffetz on the stump in American Fork, Utah ( image courtesy Michael Jolley, available under a Creative Commons-BY license) \n",
       " With apologies to Keith Olbermann, there is no doubt who the Worst Person in The World is this weekâ€“FBI Director James Comey. But according to a House Democratic aide, it looks like we also know who the second-worst person is as well. It turns out that when Comey sent his now-infamous letter announcing that the FBI was looking into emails that may be related to Hillary Clintonâ€™s email server, the ranking Democrats on the relevant committees didnâ€™t hear about it from Comey. They found out via a tweet from one of the Republican committee chairmen. \n",
       " As we now know, Comey notified the Republican chairmen and Democratic ranking members of the House Intelligence, Judiciary, and Oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. Not long after this letter went out, Oversight Committee Chairman Jason Chaffetz set the political world ablaze with this tweet. FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened \n",
       " â€” Jason Chaffetz (@jasoninthehouse) October 28, 2016 \n",
       " Of course, we now know that this was not the case . Comey was actually saying that it was reviewing the emails in light of â€œan unrelated caseâ€â€“which we now know to be Anthony Weinerâ€™s sexting with a teenager. But apparently such little things as facts didnâ€™t matter to Chaffetz. The Utah Republican had already vowed to initiate a raft of investigations if Hillary winsâ€“at least two yearsâ€™ worth, and possibly an entire termâ€™s worth of them. Apparently Chaffetz thought the FBI was already doing his work for himâ€“resulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \n",
       " But according to a senior House Democratic aide, misreading that letter may have been the least of Chaffetzâ€™ sins. That aide told Shareblue that his boss and other Democrats didnâ€™t even know about Comeyâ€™s letter at the timeâ€“and only found out when they checked Twitter. â€œDemocratic Ranking Members on the relevant committees didnâ€™t receive Comeyâ€™s letter until after the Republican Chairmen. In fact, the Democratic Ranking Members didnâ€™ receive it until after the Chairman of the Oversight and Government Reform Committee, Jason Chaffetz, tweeted it out and made it public.â€ \n",
       " So letâ€™s see if weâ€™ve got this right. The FBI director tells Chaffetz and other GOP committee chairmen about a major development in a potentially politically explosive investigation, and neither Chaffetz nor his other colleagues had the courtesy to let their Democratic counterparts know about it. Instead, according to this aide, he made them find out about it on Twitter. \n",
       " There has already been talk on Daily Kos that Comey himself provided advance notice of this letter to Chaffetz and other Republicans, giving them time to turn on the spin machine. That may make for good theater, but there is nothing so far that even suggests this is the case. After all, there is nothing so far that suggests that Comey was anything other than grossly incompetent and tone-deaf. \n",
       " What it does suggest, however, is that Chaffetz is acting in a way that makes Dan Burton and Darrell Issa look like models of responsibility and bipartisanship. He didnâ€™t even have the decency to notify ranking member Elijah Cummings about something this explosive. If that doesnâ€™t trample on basic standards of fairness, I donâ€™t know what does. \n",
       " Granted, itâ€™s not likely that Chaffetz will have to answer for this. He sits in a ridiculously Republican district anchored in Provo and Orem; it has a Cook Partisan Voting Index of R+25, and gave Mitt Romney a punishing 78 percent of the vote in 2012. Moreover, the Republican House leadership has given its full support to Chaffetzâ€™ planned fishing expedition. But that doesnâ€™t mean we canâ€™t turn the hot lights on him. After all, he is a textbook example of what the House has become under Republican control. And he is also the Second Worst Person in the World. About Darrell Lucus \n",
       " Darrell is a 30-something graduate of the University of North Carolina who considers himself a journalist of the old school. An attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right's worst nightmare--a charismatic Christian who is an unapologetic liberal. His desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. You may know him on Daily Kos as Christian Dem in NC . Follow him on Twitter @DarrellLucus or connect with him on Facebook . Click here to buy Darrell a Mello Yello. Connect,\n",
       " Ever get the feeling your life circles the roundabout rather than heads in a straight line toward the intended destination? [Hillary Clinton remains the big woman on campus in leafy, liberal Wellesley, Massachusetts. Everywhere else votes her most likely to don her inauguration dress for the remainder of her days the way Miss Havisham forever wore that wedding dress.  Speaking of Great Expectations, Hillary Rodham overflowed with them 48 years ago when she first addressed a Wellesley graduating class. The president of the college informed those gathered in 1969 that the students needed â€œno debate so far as I could ascertain as to who their spokesman was to beâ€ (kind of the like the Democratic primaries in 2016 minus the   terms unknown then even at a Seven Sisters school). â€œI am very glad that Miss Adams made it clear that what I am speaking for today is all of us â€”  the 400 of us,â€ Miss Rodham told her classmates. After appointing herself Edger Bergen to the Charlie McCarthys and Mortimer Snerds in attendance, the    bespectacled in granny glasses (awarding her matronly wisdom â€”  or at least John Lennon wisdom) took issue with the previous speaker. Despite becoming the first   to win election to a seat in the U. S. Senate since Reconstruction, Edward Brooke came in for criticism for calling for â€œempathyâ€ for the goals of protestors as he criticized tactics. Though Clinton in her senior thesis on Saul Alinsky lamented â€œBlack Power demagoguesâ€ and â€œelitist arrogance and repressive intoleranceâ€ within the New Left, similar words coming out of a Republican necessitated a brief rebuttal. â€œTrust,â€ Rodham ironically observed in 1969, â€œthis is one word that when I asked the class at our rehearsal what it was they wanted me to say for them, everyone came up to me and said â€˜Talk about trust, talk about the lack of trust both for us and the way we feel about others. Talk about the trust bust.â€™ What can you say about it? What can you say about a feeling that permeates a generation and that perhaps is not even understood by those who are distrusted?â€ The â€œtrust bustâ€ certainly busted Clintonâ€™s 2016 plans. She certainly did not even understand that people distrusted her. After Whitewater, Travelgate, the vast   conspiracy, Benghazi, and the missing emails, Clinton found herself the distrusted voice on Friday. There was a load of compromising on the road to the broadening of her political horizons. And distrust from the American people â€”  Trump edged her 48 percent to 38 percent on the question immediately prior to Novemberâ€™s election â€”  stood as a major reason for the closing of those horizons. Clinton described her vanquisher and his supporters as embracing a â€œlie,â€ a â€œcon,â€ â€œalternative facts,â€ and â€œa   assault on truth and reason. â€ She failed to explain why the American people chose his lies over her truth. â€œAs the history majors among you here today know all too well, when people in power invent their own facts and attack those who question them, it can mark the beginning of the end of a free society,â€ she offered. â€œThat is not hyperbole. â€ Like so many people to emerge from the 1960s, Hillary Clinton embarked upon a long, strange trip. From high school Goldwater Girl and Wellesley College Republican president to Democratic politician, Clinton drank in the times and the place that gave her a degree. More significantly, she went from idealist to cynic, as a comparison of her two Wellesley commencement addresses show. Way back when, she lamented that â€œfor too long our leaders have viewed politics as the art of the possible, and the challenge now is to practice politics as the art of making what appears to be impossible possible. â€ Now, as the big woman on campus but the odd woman out of the White House, she wonders how her current station is even possible. â€œWhy arenâ€™t I 50 points ahead?â€ she asked in September. In May she asks why she isnâ€™t president. The woman famously dubbed a â€œcongenital liarâ€ by Bill Safire concludes that lies did her in â€”  theirs, mind you, not hers. Getting stood up on Election Day, like finding yourself the jilted bride on your wedding day, inspires dangerous delusions.,\n",
       " Why the Truth Might Get You Fired October 29, 2016 \n",
       " The tension between intelligence analysts and political policymakers has always been between honest assessments and desired results, with the latter often overwhelming the former, as in the Iraq War, writes Lawrence Davidson. \n",
       " By Lawrence Davidson \n",
       " For those who might wonder why foreign policy makers repeatedly make bad choices, some insight might be drawn from the following analysis. The action here plays out in the United States, but the lessons are probably universal. \n",
       " Back in the early spring of 2003, George W. Bush initiated the invasion of Iraq. One of his key public reasons for doing so was the claim that the countryâ€™s dictator, Saddam Hussein, was on the verge of developing nuclear weapons and was hiding other weapons of mass destruction. The real reason went beyond that charge and included a long-range plan for â€œregime changeâ€ in the Middle East. President George W. Bush and Vice President Dick Cheney receive an Oval Office briefing from CIA Director George Tenet. Also present is Chief of Staff Andy Card (on right). (White House photo) \n",
       " For our purposes, we will concentrate on the belief that Iraq was about to become a hostile nuclear power. Why did President Bush and his close associates accept this scenario so readily? \n",
       " The short answer is Bush wanted, indeed needed, to believe it as a rationale for invading Iraq. At first he had tried to connect Saddam Hussein to the 9/11 attacks on the U.S. Though he never gave up on that stratagem, the lack of evidence made it difficult to rally an American people, already fixated on Afghanistan, to support a war against Baghdad. \n",
       " But the nuclear weapons gambit proved more fruitful, not because there was any hard evidence for the charge, but because supposedly reliable witnesses, in the persons of exiled anti-Saddam Iraqis (many on the U.S. governmentâ€™s payroll ), kept telling Bush and his advisers that the nuclear story was true. \n",
       " What we had was a U.S. leadership cadre whose worldview literally demanded a mortally dangerous Iraq, and informants who, in order to precipitate the overthrow of Saddam, were willing to tell the tale of pending atomic weapons. The strong desire to believe the tale of a nuclear Iraq lowered the threshold for proof . Likewise, the repeated assertions by assumed dependable Iraqi sources underpinned a nationwide U.S. campaign generating both fear and war fever. \n",
       " So the U.S. and its allies insisted that the United Nations send in weapons inspectors to scour Iraq for evidence of a nuclear weapons program (as well as chemical and biological weapons). That the inspectors could find no convincing evidence only frustrated the Bush administration and soon forced its hand. \n",
       " On March 19, 2003, Bush launched the invasion of Iraq with the expectation was that, once in occupation of the country, U.S. inspectors would surely find evidence of those nukes (or at least stockpiles of chemical and biological weapons). They did not. Their Iraqi informants had systematically lied to them. \n",
       " Social and Behavioral Sciences to the Rescue? \n",
       " The various U.S. intelligence agencies were thoroughly shaken by this affair, and today, 13 years later, their directors and managers are still trying to sort it out â€“ specifically, how to tell when they are getting â€œtrueâ€ intelligence and when they are being lied to. Or, as one intelligence worker has put it, we need â€œ help to protect us against armies of snake oil salesmen. â€ To that end the CIA et al. are in the market for academic assistance. Ahmed Chalabi, head of the Iraqi National Congress, a key supplier of Iraqi defectors with bogus stories of hidden WMD. \n",
       " A â€œpartnershipâ€ is being forged between the Office of the Director of National Intelligence (ODNI), which serves as the coordinating center for the sixteen independent U.S. intelligence agencies, and the National Academies of Sciences, Engineering and Medicine . The result of this collaboration will be a â€œ permanent Intelligence Community Studies Boardâ€ to coordinate programs in â€œsocial and behavioral science research [that] might strengthen national security .â€ \n",
       " Despite this effort, it is almost certain that the â€œsocial and behavioral sciencesâ€ cannot give the spy agencies what they want â€“ a way of detecting lies that is better than their present standard procedures of polygraph tests and interrogations. But even if they could, it might well make no difference, because the real problem is not to be found with the liars. It is to be found with the believers. \n",
       " The Believers \n",
       " It is simply not true, as the ODNI leaders seem to assert, that U.S. intelligence agency personnel cannot tell, more often than not, that they are being lied to. This is the case because there are thousands of middle-echelon intelligence workers, desk officers, and specialists who know something closely approaching the truth â€“ that is, they know pretty well what is going on in places like Afghanistan, Iraq, Syria, Libya, Israel, Palestine and elsewhere. Director of National Intelligence James Clapper (right) talks with President Barack Obama in the Oval Office, with John Brennan and other national security aides present. (Photo credit: Office of Director of National Intelligence) \n",
       " Therefore, if someone feeds them â€œsnake oil,â€ they usually know it. However, having an accurate grasp of things is often to no avail because their superiors â€“ those who got their appointments by accepting a pre-structured worldview â€“ have different criterion for what is â€œtrueâ€ than do the analysts. \n",
       " Listen to Charles Gaukel, of the National Intelligence Council â€“ yet another organization that acts as a meeting ground for the 16 intelligence agencies. Referring to the search for a way to avoid getting taken in by lies, Gaukel has declared, â€œ Weâ€™re looking for truth. But weâ€™re particularly looking for truth that works. â€ Now what might that mean? \n",
       " I can certainly tell you what it means historically. It means that for the power brokers, â€œtruthâ€ must match up, fit with, their worldview â€“ their political and ideological precepts. If it does not fit, it does not â€œwork.â€ So the intelligence specialists who send their usually accurate assessments up the line to the policy makers often hit a roadblock caused by â€œgroup think,â€ ideological blinkers, and a â€œwe know betterâ€ attitude. \n",
       " On the other hand, as long as what youâ€™re selling the leadership matches up with what they want to believe, you can peddle them anything: imaginary Iraqi nukes, Israel as a Western-style democracy, Saudi Arabia as an indispensable ally, Libya as a liberated country, Bashar al-Assad as the real roadblock to peace in Syria, the Strategic Defense Initiative (SDI) aka Star Wars, a world that is getting colder and not warmer, American exceptionalism in all its glory â€“ the list is almost endless. \n",
       " What does this sad tale tell us? If you want to spend millions of dollars on social and behavioral science research to improve the assessment and use of intelligence, forget about the liars. What you want to look for is an antidote to the narrow-mindedness of the believers â€“ the policymakers who seem not to be able to rise above the ideological presumptions of their class â€“ presumptions that underpin their self-confidence as they lead us all down slippery slopes. \n",
       " It has happened this way so often, and in so many places, that it is the source of Shakespeareâ€™s determination that â€œwhat is past, is prelude.â€ Our elites play out our destinies as if they have no free will â€“ no capacity to break with structured ways of seeing. Yet the middle-echelon specialists keep sending their relatively accurate assessments up the ladder of power. Hope springs eternal.]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also load Spacy's model with merged entities (which will come in handy later) and stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "en_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# Let's get the text of the news article processed by SpaCy - This might take a while depending on \n",
    "#   your hardware (a break to walk the dog? ðŸ¶)\n",
    "docs = list(tqdm(nlp.pipe(df[\"text\"], batch_size=20, n_process=cpu_count-1), total=len(df[\"text\"])))\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b2a9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9871459d26c91a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Overall, the text looks good! Not too many errors, well written... as expected from a news article. Fake news is a very tough, recent problem that is now appearing more and more frequently in the wild, usually there aren't many ortographic mistakes or slang (as it may happen with spam - another text classification problem!) since it's coming from news sources that want to be/appear credible but also clickbaity so they can profit on that good ad revenue and create distrust.\n",
    "\n",
    "Nevertheless, it is always good to process any textual information in order to normalize it, remove stopwords and punctuation so we can extract the most important parts of the text.\n",
    "\n",
    "\n",
    "## Q1. Text Cleaning\n",
    "\n",
    "#### Q1.a)\n",
    "\n",
    "With our new previously acquired knowledge, let's remove any stopwords and punctuation from our text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551e65b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abf9a07be796f9b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Hint: Remember the good old RegEx from 2 LUs ago\n",
    "        how can I just remove everything except words, digits and spaces?\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s0-9]\", \"\", text)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    \"\"\"\n",
    "    Hint: You may want to split the text into tokens using the tokenizer, it might help when searching for stopwords\n",
    "        If you do, do not forget to join the tokens afterwards!\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    return \" \".join([ w for w in tokenizer.tokenize(text) if w not in stopwords ])\n",
    "    \n",
    "    # Return the full string again here\n",
    "    # return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f49efb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5f05f229186c7541",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    df_processed[\"text\"] = df_processed[\"text\"].apply(remove_punctuation)\n",
    "    assert _hash(df_processed[\"text\"].values) == \"9c34086ca91f5845a1069878dd4fd7fcf54826bdf02a0240f644b78257b73137\", \\\n",
    "        \"it appears you are not removing all of the punctuation, read the hint ðŸ˜‰.\"\n",
    "    \n",
    "    \n",
    "    df_processed[\"text\"] = df_processed[\"text\"].apply(remove_stopwords, stopwords = en_stopwords)\n",
    "    assert _hash(df_processed[\"text\"].values) == \"e10c9b012ef768908431c03fdc7bf0ae6b11cd2004397f93a9bd262b5d432b8f\", \\\n",
    "        \"something wrong with removing stopwords, read the hints!\"\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "df_processed = preprocess_text(df)\n",
    "assert df_processed.shape == (5000, 2), \"something is wrong with the shape of the dataframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0634e62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2777339c1d76131e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.b)\n",
    "\n",
    "With our text processed, let's get a baseline model for our classification problem! Let's use our comfortable _TfidfVectorizer_ to get a simple, fast and trustworthy baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e900b1d2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1cf7d1d751604527",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def baseline_with_tfidf(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a Random Forest using sklearn's Pipeline and return the trained model and its accuracy in the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    pipe = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', RandomForestClassifier())])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = np.mean(y_pred == y_test)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b79669",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c345c9379c620ae2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_processed[\"text\"], df_processed[\"label\"], \n",
    "                                                    test_size=0.2, random_state=42, stratify=df_processed[\"label\"])\n",
    "baseline_model, baseline_acc = baseline_with_tfidf(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# asserts\n",
    "assert isinstance(baseline_model, Pipeline)\n",
    "assert _hash(baseline_model[0]) == \"5d9dc3620e12f84e4f957a7b00db14e15ebcc5d20cbc1f883940318fbb5442d5\", \"Something\\\n",
    "is wrong! Use the default parameters!\"\n",
    "assert _hash(baseline_model[1]) == \"7ab1fd7f03f247b36ba389a0a2eb8767ed2f1d2535f8e295669ac5ae2319d3c8\", \"Something\\\n",
    "is wrong! Use the default parameters!\"\n",
    "assert np.allclose(baseline_acc, 0.908, 0.01), \"something wrong with the accuracy score. Use the default parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc804ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ecc682a6b13cabf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wow, the accuracy is quite good for such a simple text model! This just proves that, a starting trustworthy baseline is all you need. I can't stress enough that it's really important to have a simple first iteration, and afterwards we can add complexity and study which features do make sense or not, testing more out of the box solutions. \n",
    "\n",
    "Sometimes, data scientists focus right off the bat on the most complex solutions and a simple one would be enough. Real life problems will obviously achieve lower scores as the datasets are not controlled or cleaned for you but that should not stop you from starting with a simpler and easier solution.\n",
    "\n",
    "Now let's see if adding new features we can still improve our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3f611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9acda58125c99fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2. SpaCy Matcher\n",
    "\n",
    "Let's see if we can extract some useful features by using our SpaCy Matcher.\n",
    "\n",
    "#### Q2.a) Simple Matcher\n",
    "\n",
    "You think of some words that could be related with the detection of Fake News. Something starts ringing in your mind about \"propaganda\", \"USA\" and \"fraud\", so you decide to check how many of those words appear in our news articles using the SpaCy Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f158c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca5f9a618bc32ee6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words = [\"propaganda\", \"USA\", \"fraud\"]\n",
    "\n",
    "# init the matcher - remember it from the learning notebook\n",
    "# add the patterns of the words. HINT: for a direct match you need a specific pattern (check SpaCy docs)\n",
    "# count how many matches!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for word in words:\n",
    "    pattern = [[{'ORTH': word}]]\n",
    "    matcher.add(word, pattern)\n",
    "\n",
    "    \n",
    "count = 0   \n",
    "for i, doc in enumerate(docs):\n",
    "    count += len(matcher(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6002d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-57893459c2e5a1ac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert _hash(count) == \"47fec9f491173c57c1d5b35dfefdb69cba6bd61bfbadea64015a65120efa15a0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374a2f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9a8e7ce0cab87fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.b) POS-Tagging Search\n",
    "\n",
    "Your head is still working new theories. You start thinking that, fake news might exaggerate on adjectives and adverbs by sharing exaggerated or over the top descriptions. So you decide to create a feature that counts the number of _Adjectives_ and _Adverbs_ in a piece of news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c170ff4c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d048ae4dde4a43cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HINT: you already have your news text processed (the docs variable),\n",
    "# so you can go over every doc and check if there is any POS Tag which is an ADJ or ADV\n",
    "# to check the POS tag of a token in a doc -----> token.pos_\n",
    "\n",
    "\"\"\"\n",
    "Try it out by running the below code! \n",
    "for token in docs[0]:\n",
    "    print(token.pos_)\n",
    "\"\"\"\n",
    "\n",
    "# Return a list with the number of adjectives and adverbs for every piece of news in docs\n",
    "# nb_adj_adv = [...]\n",
    "\n",
    "# YOUR CODE HERE  \n",
    "nb_adj_adv = []    \n",
    "for doc in docs:\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' or token.pos_ == 'ADV':\n",
    "            count += 1\n",
    "\n",
    "    nb_adj_adv.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4618744a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3961c5c5cb9b18aa",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(nb_adj_adv) == list, \"the variable should be a list with just 1 dimension.\"\n",
    "assert len(nb_adj_adv) == 5000, \"the length of the array is wrong. You should have a count for every news article.\"\n",
    "\n",
    "value_hash = \"2488c9b42fd6efc018e0857683cc782347c4a09763a5d94579ca41425d4b6f64\"\n",
    "assert _hash(nb_adj_adv) == value_hash\n",
    "df_processed[\"nb_adj_adv\"] = nb_adj_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452de0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54363f2d61151f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.c) Entity Search\n",
    "\n",
    "Another theory that might be worth testing is that people and organizations are often involved in this kind of news. Nowadays, a lot of fake news are often shared by these to justify or divert attention to/from their actions. You think that, another smart feature could be to analyse if there are any known identities (people and/or organizations) that might be closely related with fake news.\n",
    "\n",
    "In order to do this, you decide to create a Matcher that searches for _People_ and identifies which are the ones that appear most frequently in our piece of news. **Let's find the top 10!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a89fe123",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a5547e1de72b597",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Trump', 5136), ('Clinton', 3379), ('Obama', 2608), ('Hillary Clinton', 1830), ('Donald Trump', 1580), ('Hillary', 1502), ('Twitter', 572), ('Putin', 544), ('Barack Obama', 364), ('Donald J. Trump', 352)]\n"
     ]
    }
   ],
   "source": [
    "# I'll reset the matcher for you\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[{\"ENT_TYPE\": \"PERS\"}]] # to find people entities\n",
    "matcher.add(\"mymatcher\", pattern)\n",
    "\n",
    "matches = [ matcher(doc) for doc in docs]\n",
    "\n",
    "#print(match_id)\n",
    "#print(start)\n",
    "#print(end)\n",
    "#print(docs[0][start:end])\n",
    "#print(docs[0])\n",
    "\n",
    "mydict = {}\n",
    "#for doc in docs:\n",
    "#    matches = matcher(doc)\n",
    "#    for match_id, start, end in matches:\n",
    "for i, doc in enumerate(docs):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'PERSON': # or e.label_ == 'ORG':  \n",
    "            key = e.text\n",
    "\n",
    "            if key not in mydict:\n",
    "                mydict[key] = 0\n",
    "\n",
    "            mydict[key] = mydict[key] + 1\n",
    "\n",
    "#for doc in docs:\n",
    "    # do matches and save the text in a list\n",
    "    \n",
    "\n",
    "# count the number of times the same Person appears on the list (hint: remember the dictionary solution...)\n",
    "# only take the top 10 of the counter! THE RESULT SHOULD BE A LIST\n",
    "\n",
    "most_common_ents = sorted(mydict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "#print(most_common_ents)\n",
    "\n",
    "#most_common_ents = [ x[0] for x in most_common_ents ] \n",
    "\n",
    "\n",
    "print(most_common_ents)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6de656a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a55276f7a5cb1f58",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(most_common_ents) == list, \"the output is not a list\"\n",
    "assert len(most_common_ents) == 10, \"It should be the highest 10 people!\"\n",
    "\n",
    "value_hash = \"95983e691298df0238a6d47e54fff172a5166924dfdad865a3ce4f6ac6c52cf6\"\n",
    "assert _hash(most_common_ents) == value_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943cdd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c9dcb2ac4fd3aff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Well, now I'm curious to see who is on the top 10. Since this dataset is from the USA, I think we can already deduce who is going to show up in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf32b64a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c68d93893da9985e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Trump', 5136),\n",
       " ('Clinton', 3379),\n",
       " ('Obama', 2608),\n",
       " ('Hillary Clinton', 1830),\n",
       " ('Donald Trump', 1580),\n",
       " ('Hillary', 1502),\n",
       " ('Twitter', 572),\n",
       " ('Putin', 544),\n",
       " ('Barack Obama', 364),\n",
       " ('Donald J. Trump', 352)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3e265",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5c997aef8ff2a9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As expected, we have some known names here. The matcher was also able to detect full names and join then in a single occurrence (when they appeared together in the sentence). This was only possible since we called the following line\n",
    "\n",
    "`nlp.add_pipe(\"merge_entities\", after=\"ner\")`\n",
    "\n",
    "before processing the documents with SpaCy. If we didn't, every name would be considered independent even when belonging to the same person.\n",
    "\n",
    "We can also check how many times do these people appear for each label of news!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78e5cfe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c52187cad7e360a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Clinton\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Obama\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Hillary Clinton\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Donald Trump\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Hillary\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Twitter\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Putin\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Barack Obama\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Donald J. Trump\n",
      "1    2532\n",
      "0    2468\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for person, _ in most_common_ents:\n",
    "    print(person)\n",
    "    print(df[df['text'].str.contains(\"\")].label.value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79678441",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5de1f6e3a47a8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "From the distribution it might not be a useful feature at all :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244da23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59afa572e05bc308",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3. Feature Unions\n",
    "\n",
    "Now the only thing missing is to create a Feature Union that allows us to join the features we have so far and see if we can actually improve our baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc10835e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61e270fd37131ec7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca16469",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692f18d2b61996fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q3.a) Adding Extra Features\n",
    "\n",
    "First off, there are some simple features that we can extract from the dataset to try and to enrich our model! Let's add to our dataframe the following features: **number of words in the doc**, **length of the doc** and **average word length**. Remember we already have the **number of adjectives and adverbs** that we also want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acb93e53",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6910256a13fa82fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafael.gil/.virtualenvs/blu9/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_processed[\"nb_words\"] = df_processed[\"text\"].map(lambda x: len(x.split()))\n",
    "df_processed[\"doc_length\"] = df_processed[\"text\"].map(lambda x: len(x))\n",
    "df_processed[\"avg_word_length\"] = df_processed['text'].apply(lambda x: np.mean([len(t) for t in x.split()]) if len([len(t) for t in x.split(' ')]) > 0 else 0)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "#df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ff5b354",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-31106be84628e3c9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert df_processed.shape == (5000, 6), \"Something wrong about the shape, do you have all columns/rows?\"\n",
    "assert \"nb_words\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"doc_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_word_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "\n",
    "hash_nb_words = \"bbd0f5a5179c2e0433c3cfd2bf5809c4db8f3b7dfdf44a6729c79c9337ff2361\"\n",
    "hash_doc_length = \"3fa5e413714a16c6c9b463ff9883f366dd8fd8e4e46812bdb589365b4afbe54d\"\n",
    "hash_avg_word_length = \"9c11f12992d183e81f7c20ebc3e901fc4b2ef56ab01d2d2046f0603f70abb043\"\n",
    "\n",
    "assert _hash(df_processed[\"nb_words\"]) == hash_nb_words, \"Something wrong with how you are calculating this column.\"\n",
    "assert _hash(df_processed[\"doc_length\"]) == hash_doc_length, \"Something wrong with how you are calculating this column.\"\n",
    "assert _hash(df_processed[\"avg_word_length\"]) == hash_avg_word_length, \"Something wrong with how you are calculating this column.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1e3de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b12d83fd4d036457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Q3 b) Feature Union\n",
    "\n",
    "Let's create a processing _Pipeline_ for every new feature and then join them all using a _Feature Union_. For the textual feature use the usual _TfidfVectorizer_ with default parameters and for any numerical feature use a _Standard Scaler_. Afterwards, join the features pipelines using a _Feature Union_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "726e0bf1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a5c8a9abba2b5e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "text_pipe = Pipeline([\n",
    "                ('selector', TextSelector(\"text\")),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])\n",
    "\n",
    "nb_adj_adv_pipe = Pipeline([\n",
    "                ('selector', NumberSelector(\"nb_adj_adv\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "nb_words_pipe = Pipeline([\n",
    "                ('selector', NumberSelector(\"nb_words\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "doc_length_pipe = Pipeline([\n",
    "                ('selector', NumberSelector(\"doc_length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "avg_word_length_pipe = Pipeline([\n",
    "                ('selector', NumberSelector(\"avg_word_length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "feats = FeatureUnion([('text', text_pipe), \n",
    "                      ('nb_adj_adv', nb_adj_adv_pipe),\n",
    "                      ('nb_words', nb_words_pipe),\n",
    "                      ('doc_length', doc_length_pipe),\n",
    "                      ('avg_word_length', avg_word_length_pipe)])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "402bc439",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-98151e411b73b1f5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert len(feats.transformer_list) == 5, \"Are you creating 5 pipelines? One for each feature?\"\n",
    "for pipe in feats.transformer_list:\n",
    "    \n",
    "    selector = pipe[1][0]\n",
    "    if not (isinstance(selector, TextSelector) or isinstance(selector, NumberSelector)):\n",
    "        raise AssertionError(\"pipeline is wrong, the Selectors should come first.\")\n",
    "        \n",
    "    feature_builder = pipe[1][1]\n",
    "    if not (isinstance(feature_builder, TfidfVectorizer) or isinstance(feature_builder, StandardScaler)):\n",
    "        raise AssertionError(\"pipeline is wrong, the second thing to come should be the Tfidf or the Scaler.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb7c8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8c92614d69d640a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's build our function to use our newly created _Feature Union_ and calculate its performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d431c958",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb15cc415e6737d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improved_pipeline(feats, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a Random Forest using sklearn's Pipeline and return the trained model and its accuracy in the test set.\n",
    "    Don't forget to add the feats to the Pipeline!\n",
    "    \"\"\"\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('features',feats),\n",
    "        ('classifier', RandomForestClassifier()),\n",
    "    ])\n",
    "    \n",
    "    X_train_fill = X_train.copy()\n",
    "    X_test_fill = X_test.copy()\n",
    "    \n",
    "    X_train_fill[\"avg_word_length\"] = X_train_fill[\"avg_word_length\"].fillna(0)\n",
    "    X_test_fill[\"avg_word_length\"] = X_test_fill[\"avg_word_length\"].fillna(0)    \n",
    "    \n",
    "    pipe.fit(X_train_fill, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test_fill)\n",
    "    acc = np.mean(y_pred == y_test)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04da0430",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-873e3b1b663a7b12",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Y = df_processed[\"label\"]\n",
    "X = df_processed.drop(columns=\"label\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "pipeline_model, pipeline_acc = improved_pipeline(feats, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# asserts\n",
    "assert isinstance(pipeline_model, Pipeline)\n",
    "assert _hash(pipeline_model[0]) == \"f5e738d891cee945082226770873c560481fccce687af07ea966b30de065ac35\", \"The first part of the\\\n",
    "Pipeline is incorrect.\"\n",
    "assert _hash(pipeline_model[1]) == \"7ab1fd7f03f247b36ba389a0a2eb8767ed2f1d2535f8e295669ac5ae2319d3c8\", \"The second part of the\\\n",
    "Pipeline is incorrect.\"\n",
    "assert np.allclose(pipeline_acc, 0.896, 0.03), \"something wrong with the accuracy score. Use the default parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e0258",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec9f67cb4f0f5b43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With this more complex approach we have achieved a very similar performance when compared to our baseline. This might mean a lot of things: our features might have no real revelance to the model (which you will learn how to check later on with some feature importance) or we have achieved a plateau and can't improve the score with the this technique. \n",
    "\n",
    "Nevertheless it is a good score for this problem and dataset. Regardless you have learnt a lot about _POS Tagging_, _Entities_, _Feature Union_ and also leant that the sky is the limit when creating features. anything can be a feature really - now good features are a totally different thing that might need more research and validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
